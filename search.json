[{"title":"Spark(1)——wordcount","path":"/2024/02/18/Spark(1)——wordcount/","content":"Spark的wordCount会明显比MapReduce简单非常多，MR程序需要实现map和reduce接口，还需要一个job入口，但是Spark通过内置的RDD算子，可以非常简单的实现原本复杂的代码 wordCount123456789101112131415161718192021222324252627282930313233343536373839404142package org.example;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import scala.Tuple2;import java.util.Arrays;public class Main &#123; public static void main(String[] args) &#123; //创建jsc，相当于scala中的sparkContext //本地运行setMaster=local,如提交yarn集群，则local改为yarn SparkConf conf=new SparkConf().setAppName(&quot;wordcount&quot;).setMaster(&quot;local&quot;); JavaSparkContext jsc=new JavaSparkContext(conf); String filePath=&quot;D:\\\\MyConfiguration\\\\yang24.wang\\\\Desktop\\\\sparkTest.txt&quot;; //读取文件 //默认按行读取 JavaRDD&lt;String&gt; lineRDD = jsc.textFile(filePath); //调用flatMap算子，将基本单位从line装化成word //注意java中lambda表达式的语法，不像scala中直接 //scala语法: val wordRDD: RDD[String] = lineRDD.flatMap(line =&gt; line.split(&quot; &quot;))// JavaRDD&lt;String&gt; wordRDD = lineRDD.flatMap(line -&gt;Arrays.asList(line.split(&quot; &quot;)).iterator()); JavaRDD&lt;String&gt; wordRDD = lineRDD.flatMap(line-&gt; Arrays.stream(line.split(&quot; &quot;)).iterator()); //过滤空格字符 JavaRDD&lt;String&gt; cleanWordRDD = wordRDD.filter(word -&gt; !word.isEmpty()); //RDD-&gt;pairRDD，注意这里用mapToPair，lambda表达式语法，引入Tuple2 //scala: val kvRDD: RDD[(String, Int)] = cleanWordRDD.map(word =&gt; (word, 1)) JavaPairRDD&lt;String, Integer&gt; pairRDD = cleanWordRDD.mapToPair(word -&gt; new Tuple2&lt;&gt;(word, 1)); JavaPairRDD&lt;String, Integer&gt; wordCount = pairRDD.reduceByKey((a, b) -&gt; a + b); //保存到项目根目录下的wordcount文件夹 wordCount.saveAsTextFile(&quot;wordcount&quot;); &#125;&#125; RDD算子 创建RDD12345678910111213141516171819202122232425262728package org.example;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import java.util.ArrayList;import java.util.Arrays;public class Main &#123; public static void main(String[] args) &#123; SparkConf conf=new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;test&quot;); JavaSparkContext jsc=new JavaSparkContext(conf); ArrayList&lt;String&gt; arrayList=new ArrayList&lt;&gt;(); arrayList.add(&quot;spark&quot;); arrayList.add(&quot;is&quot;); arrayList.add(&quot;good&quot;); //内部数据用jsc.parallelize创建 JavaRDD&lt;String&gt; arrayRDD = jsc.parallelize(arrayList); //外部数据用jsc.textFile创建 JavaRDD&lt;String&gt; stringJavaRDD = jsc.textFile(&quot;D:\\\\MyConfiguration\\\\yang24.wang\\\\Desktop\\\\wikiOfSpark.txt&quot;); &#125;&#125; RDD 内数据转换map：以元素为粒度的数据转换 12345678910111213141516171819202122package org.example;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import java.util.Arrays;import java.util.List;public class Main &#123; public static void main(String[] args) &#123; SparkConf conf=new SparkConf().setAppName(&quot;wordcount&quot;).setMaster(&quot;local&quot;); JavaSparkContext jsc=new JavaSparkContext(conf); List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 5, 6, 7); JavaRDD&lt;Integer&gt; rdd = jsc.parallelize(list); JavaRDD&lt;Integer&gt; map = rdd.map(a -&gt; a * a); map.collect().forEach(System.out::println); &#125;&#125; ……","categories":["大数据","Spark"]},{"title":"Flink(3)——状态","path":"/2024/02/17/Flink(3)——状态/","content":"什么是状态flink算子可以分为有状态和无状态 无状态的算子任务只需要观察每个独立事件，根据当前输入的数据直接转换输出结果。像基本转换算子，如map、filter、flatMap，计算时不依赖其他数据，就都属于无状态的算子。 而有状态的算子任务，则除当前数据之外，还需要一些其他数据来得到计算结果。这里的“其他数据”，就是所谓的状态（state）。我们之前讲到的算子中，聚合算子、窗口算子都属于有状态的算子。 有状态算子的一般处理流程，具体步骤如下 算子任务收到上游数据 获取当前状态 根据业务逻辑进行计算，更新状态 得到计算结果，输出发送到下游任务 状态的分类 托管状态 算子状态 按键分区状态 原始状态 暂不讨论 keyed state又分为ValueState，ListState，MapState，ReducingState，AggregatingState 以ValueState为例 ValueState顾名思义，状态就是一个值 直接上案例：连续的两个数字相等就输出该数字 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package org.example.env;import org.apache.flink.api.common.state.ValueState;import org.apache.flink.api.common.state.ValueStateDescriptor;import org.apache.flink.api.common.typeinfo.Types;import org.apache.flink.configuration.Configuration;import org.apache.flink.runtime.state.hashmap.HashMapStateBackend;import org.apache.flink.streaming.api.CheckpointingMode;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.KeyedStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.datastream.WindowedStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.KeyedProcessFunction;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.WindowAssigner;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;import java.util.Arrays;import java.util.List;public class env &#123; public static void main(String[] args) throws Exception &#123; //获取执行环境 StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment(); //设置并行度为1 env.setParallelism(1); //获取流数据 DataStreamSource&lt;String&gt; source = env.socketTextStream(&quot;node1&quot;, 7777); SingleOutputStreamOperator&lt;String&gt; process = source.keyBy(a -&gt; &quot;1&quot;).process(new KeyedProcessFunction&lt;String, String, String&gt;() &#123; private ValueState&lt;String&gt; valueState; @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); ValueStateDescriptor&lt;String&gt; des = new ValueStateDescriptor&lt;&gt;(&quot;lastValue&quot;, Types.STRING); valueState = getRuntimeContext().getState(des); &#125; @Override public void processElement(String value, KeyedProcessFunction&lt;String, String, String&gt;.Context ctx, Collector&lt;String&gt; out) throws Exception &#123; String lastValue = valueState.value(); if (value.equals(lastValue)) &#123; System.out.println(&quot;连续的两次输入相等:&quot; + value); &#125; valueState.update(value); &#125; &#125;); process.print(); //执行 env.execute(); &#125;&#125;","categories":["大数据","Flink"]},{"title":"Flink(2)——时间与窗口","path":"/2024/02/17/Flink(2)——时间与窗口/","content":"在批处理统计中，我们是等待一批数据都到齐后，统一处理。但是在实时处理统计中，我们是来一条就得处理一条，但是单条的意义很有限，我们会更想要统计一段时间内的数据。 这就需要引入“窗口”。 所谓的“窗口”，一般就是划定的一段时间范围，也就是“时间窗”；对在这范围内的数据进行处理，就是所谓的窗口计算。所以窗口和时间往往是分不开的。 本文将带大家了解一下Flink中的时间和窗口的应用。 主要分为4部分： 时间语义：简单介绍flink中的时间语义 窗口：介绍窗口的概念和API 水位线：flink中的水位线概念 案例：最后完成一个双流Join的案例收尾 主要是对 尚硅谷大数据Flink1.17实战教程从入门到精通 相关章节的整理 1. 时间语义因为存在网络延迟，所以flink中的事件时间和处理时间是不一样的 到底以哪一种时间作为衡量标准，就是所谓的“时间语义” 在实际应用中，事件时间语义会更为常见。一般情况下，业务日志数据中都会记录数据生成的时间戳（timestamp），它就可以作为事件时间的判断基础。 在Flink中，由于处理时间比较简单，早期版本默认的时间语义是处理时间；而考虑到事件时间在实际应用中更为广泛，从Flink1.12版本开始，Flink已经将事件时间作为默认的时间语义了。 2. 窗口（window）2.1 窗口的概念Flink是一种流式计算引擎，主要是来处理无界数据流的，数据源源不断、无穷无尽。想要更加方便高效地处理无界流，一种方式就是将无限数据切割成有限的“数据块”进行处理，这就是所谓的“窗口”（Window） 注意：Flink中窗口并不是静态准备好的，而是动态创建——当有落在这个窗口区间范围的数据达到时，才创建对应的窗口。另外，这里我们认为到达窗口结束时间时，窗口就触发计算并关闭，事实上“触发计算”和“窗口关闭”两个行为也可以分开，这部分内容我们会在后面详述。 2.2 窗口的分类窗口的应用非常灵活，我们可以使用各种不同类型的窗口来实现需求。接下来我们就从不同的角度，对Flink中内置的窗口做一个分类说明。 1）按照驱动类型分类 时间窗口： 计数窗口： 2）按照窗口分配数据的规则分类 根据分配数据的规则，窗口的具体实现可以分为4类： 滚动窗口（Tumbling Window） 窗口大小固定，窗口之间无重叠、无间隔 比如：每10分钟统计10分钟内所有订单数量，窗口大小就是10分钟，窗口之间无重叠、无间隔 滑动窗口（Sliding Window） 窗口大小固定，但是窗口之间并非如滚动窗口一样首尾相接，存在重叠，称之为滑动步长 比如：每10分钟统计1个小时内的所有订单数量，窗口大小就是1小时，窗口之间有重叠，滑动步长10分钟 滚动窗口可以看成一种特殊的滑动窗口——窗口大小&#x3D;滑动步长 会话窗口（Session Window） 以及全局窗口（Global Window） 2.3 窗口API概览在定义窗口操作之前，首先需要确定，到底是基于按键分区（Keyed）的数据流KeyedStream来开窗，还是直接在没有按键分区的DataStream上开窗。也就是说，在调用窗口算子之前，是否有keyBy操作。 按键分区窗口（Keyed Windows） 经过按键分区keyBy操作后，数据流会按照key被分为多条逻辑流（logical streams），这就是KeyedStream。基于KeyedStream进行窗口操作时，窗口计算会在多个并行子任务上同时执行。相同key的数据会被发送到同一个并行子任务，而窗口操作会基于每个key进行单独的处理。所以可以认为，每个key上都定义了一组窗口，各自独立地进行统计计算。 非按键分区（Non-Keyed Windows） 如果没有进行keyBy，那么原始的DataStream就不会分成多条逻辑流。这时窗口逻辑只能在一个任务（task）上执行，就相当于并行度变成了1 注意：对于非按键分区的窗口操作，手动调大窗口算子的并行度也是无效的，windowAll本身就是一个非并行的操作。 12345678910111213//按键分区和非按键分区//并行度=1listDataStreamSource.windowAll(……);//每个key上都有一组窗口listDataStreamSource.keyBy(a-&gt;a).window(……);//窗口操作主要有两个部分：窗口分配器（Window Assigners）和窗口函数（Window Functions）//.window()方法需要传入一个窗口分配器，它指明了窗口的类型//.aggregate()方法传入一个窗口函数作为参数，它用来定义窗口具体的处理逻辑//窗口分配器有各种形式，而窗口函数的调用方法也不只.aggregate()一种，后文详细展开stream.keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .aggregate(&lt;window function&gt;) 2.4 窗口分配器定义窗口分配器（Window Assigners）是构建窗口算子的第一步，它的作用就是定义数据应该被“分配”到哪个窗口。所以可以说，窗口分配器其实就是在指定窗口的类型。 窗口分配器最通用的定义方式，就是调用.window()方法。这个方法需要传入一个WindowAssigner作为参数，返回WindowedStream。如果是非按键分区窗口，那么直接调用.windowAll()方法，同样传入一个WindowAssigner，返回的是AllWindowedStream。 123stream.keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .aggregate(&lt;window function&gt;) 窗口按照驱动类型可以分成时间窗口和计数窗口，而按照具体的分配规则，又有滚动窗口、滑动窗口、会话窗口、全局窗口四种。除去需要自定义的全局窗口外，其他常用的类型Flink中都给出了内置的分配器实现，我们可以方便地调用实现各种需求。 2.4.1 时间窗口时间窗口是最常用的窗口类型，又可以细分为滚动、滑动和会话三种，结合两种时间语义，共有六种时间窗口 （1）滚动处理时间窗口 （2）滑动处理时间窗口 （3）会话处理时间窗口 （4）滚动事件时间窗口 窗口分配器由类TumblingEventTimeWindows提供，用法与滚动处理事件窗口完全一致。 123stream.keyBy(...) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .aggregate(...) （5）滑动事件时间窗口 窗口分配器由类SlidingEventTimeWindows提供，用法与滑动处理事件窗口完全一致。 123stream.keyBy(...) .window(SlidingEventTimeWindows.of(Time.seconds(10)，Time.seconds(5))) .aggregate(...) （6）会话事件时间窗口 窗口分配器由类EventTimeSessionWindows提供，用法与处理事件会话窗口完全一致。 123stream.keyBy(...) .window(EventTimeSessionWindows.withGap(Time.seconds(10))) .aggregate(...) 2.4.2 计数窗口计数窗口概念非常简单，本身底层是基于全局窗口（Global Window）实现的。Flink为我们提供了非常方便的接口：直接调用.countWindow()方法。根据分配规则的不同，又可以分为滚动计数窗口和滑动计数窗口两类，下面我们就来看它们的具体实现。 （1）滚动计数窗口 滚动计数窗口只需要传入一个长整型的参数size，表示窗口的大小。 12stream.keyBy(...) .countWindow(10) 我们定义了一个长度为10的滚动计数窗口，当窗口中元素数量达到10的时候，就会触发计算执行并关闭窗口。 （2）滑动计数窗口 与滚动计数窗口类似，不过需要在.countWindow()调用时传入两个参数：size和slide，前者表示窗口大小，后者表示滑动步长。 12stream.keyBy(...) .countWindow(10，3) 我们定义了一个长度为10、滑动步长为3的滑动计数窗口。每个窗口统计10个数据，每隔3个数据就统计输出一次结果。 注意：第一次输出是在第三个数据来时触发，而不是第10个 例如：0,1,2,3……9,在0,1,2之后就触发窗口输出 3）全局窗口 全局窗口是计数窗口的底层实现，一般在需要自定义窗口时使用。它的定义同样是直接调用.window()，分配器由GlobalWindows类提供。 12stream.keyBy(...) .window(GlobalWindows.create()); 需要注意使用全局窗口，必须自行定义触发器才能实现窗口计算，否则起不到任何作用。 2.5 窗口函数 增量聚合函数（reduce、aggregate）窗口将数据收集起来，最基本的处理操作当然就是进行聚合。我们可以每来一个数据就在之前结果上聚合一次，这就是“增量聚合” 典型的增量聚合函数有两个：ReduceFunction和AggregateFunction。 reduceFunction123456789101112131415161718192021222324252627282930313233343536373839404142package org.example.window;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.KeyedStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.datastream.WindowedStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.WindowAssigner;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;public class window &#123; public static void main(String[] args) throws Exception &#123; //获取执行环境 StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment(); //设置并行度为2 env.setParallelism(2); //获取流数据 DataStreamSource&lt;String&gt; source = env.socketTextStream(&quot;node1&quot;, 7777); SingleOutputStreamOperator&lt;Integer&gt; map = source.map(a -&gt; Integer.parseInt(a)); //按照奇偶分组，同一组一定在同一个分区 KeyedStream&lt;Integer, Integer&gt; keyedStream = map.keyBy(a -&gt; a % 2); //设置5秒的窗口 WindowedStream&lt;Integer, Integer, TimeWindow&gt; window = keyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(5))); //reduce function，简单的累加操作 SingleOutputStreamOperator&lt;Integer&gt; reduced = window.reduce((a, b) -&gt; a + b); reduced.print(); //执行 env.execute(); &#125;&#125; 一般来讲，ReduceFunction可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类型、输出结果的类型都必须和输入数据类型一样。 而aggregate就突破了这个限制，可以定义更加灵活的窗口聚合操作。这个方法需要传入一个AggregateFunction的实现类作为参数。 aggregateFunctionAggregateFunction可以看作是ReduceFunction的通用版本，这里有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。输入类型IN就是输入流中元素的数据类型；累加器类型ACC则是我们进行聚合的中间状态类型；而输出类型当然就是最终计算结果的类型了。 接口中有四个方法： createAccumulator()：创建一个累加器，这就是为聚合创建了一个初始状态，每个聚合任务只会调用一次。 add()：将输入的元素添加到累加器中。 getResult()：从累加器中提取聚合的输出结果。 merge()：合并两个累加器，并将合并后的状态作为一个累加器返回。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package org.example.window;import org.apache.flink.api.common.functions.AggregateFunction;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.KeyedStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.datastream.WindowedStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.WindowAssigner;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;public class window &#123; public static void main(String[] args) throws Exception &#123; //获取执行环境 StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment(); //设置并行度为2 env.setParallelism(2); //获取流数据 DataStreamSource&lt;String&gt; source = env.socketTextStream(&quot;node1&quot;, 7777); SingleOutputStreamOperator&lt;Integer&gt; map = source.map(a -&gt; Integer.parseInt(a)); //按照奇偶分组，同一组一定在同一个分区 KeyedStream&lt;Integer, Integer&gt; keyedStream = map.keyBy(a -&gt; a % 2); //设置5秒的窗口 WindowedStream&lt;Integer, Integer, TimeWindow&gt; window = keyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(5))); SingleOutputStreamOperator&lt;Integer&gt; aggregate = window.aggregate( /** * 三个变量：IN，ACC，OUT */ new AggregateFunction&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer createAccumulator() &#123; //初始化acc System.out.println(&quot;createAcc&quot;); return 0; &#125; @Override public Integer add(Integer value, Integer accumulator) &#123; //这里的add和reduce的add不同，因为两个参数是value和acc，而reduce的两个参数都是value，所以reduce要等第二个输入进入才触发add，但是agg是一个输入就可以触发add System.out.println(&quot;add&quot;); return value + accumulator; &#125; @Override public Integer getResult(Integer accumulator) &#123; System.out.println(&quot;getResult&quot;); return accumulator; &#125; @Override public Integer merge(Integer a, Integer b) &#123; System.out.println(&quot;merge&quot;); return null; &#125; &#125;); aggregate.print(); //执行 env.execute(); &#125;&#125; 所以可以看到，AggregateFunction的工作原理是： 首先调用createAccumulator()为任务初始化一个状态（累加器）； 而后每来一个数据就调用一次add()方法，对数据进行聚合，得到的结果保存在状态中； 等到了窗口需要输出时，再调用getResult()方法得到计算结果。 与ReduceFunction相同，AggregateFunction也是增量式的聚合；但由于输入、中间状态、输出的类型可以不同，使得应用更加灵活方便。 预定义的简单聚合方法Flink也为窗口的聚合提供了一系列预定义的简单聚合方法，可以直接基于WindowedStream调用。主要包括.sum()&#x2F;max()&#x2F;maxBy()&#x2F;min()&#x2F;minBy()，与KeyedStream的简单聚合非常相似。它们的底层，其实都是通过AggregateFunction来实现的。 123456789101112131415161718192021222324252627282930313233343536373839404142package org.example.window;import org.apache.flink.api.common.functions.AggregateFunction;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.KeyedStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.datastream.WindowedStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.WindowAssigner;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;public class window &#123; public static void main(String[] args) throws Exception &#123; //获取执行环境 StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment(); //设置并行度为2 env.setParallelism(2); //获取流数据 DataStreamSource&lt;String&gt; source = env.socketTextStream(&quot;node1&quot;, 7777); SingleOutputStreamOperator&lt;Integer&gt; map = source.map(a -&gt; Integer.parseInt(a)); //按照奇偶分组，同一组一定在同一个分区 KeyedStream&lt;Integer, Integer&gt; keyedStream = map.keyBy(a -&gt; a % 2); //设置10秒的窗口 WindowedStream&lt;Integer, Integer, TimeWindow&gt; window = keyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(10))); //max传入的参数表示比较列的下标 SingleOutputStreamOperator&lt;Integer&gt; max = window.max(0); max.print(); //执行 env.execute(); &#125;&#125; 全窗口函数有些场景下，我们要做的计算必须基于全部的数据才有效，这时做增量聚合就没什么意义了；另外，输出的结果有可能要包含上下文中的一些信息（比如窗口的起始时间），这是增量聚合函数做不到的。 所以，我们还需要有更丰富的窗口计算方式。窗口操作中的另一大类就是全窗口函数。与增量聚合函数不同，全窗口函数需要先收集窗口中的数据，并在内部缓存起来，等到窗口要输出结果的时候再取出数据进行计算。 例如：1+1+1+1+1+1 增量聚合是不断累加，1+1&#x3D;2+1&#x3D;3+1&#x3D;4+1&#x3D;5 全窗口是先把窗口中的数据缓存起来，一起计算：1+1+1+1+1&#x3D;5 在Flink中，全窗口函数也有两种：WindowFunction和ProcessWindowFunction。 不过WindowFunction能提供的上下文信息较少，也没有更高级的功能，逐渐被processWindowFunction淘汰，本文主要介绍processwindowfunction 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package org.example.window;import org.apache.commons.lang3.time.DateFormatUtils;import org.apache.flink.api.common.functions.AggregateFunction;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.KeyedStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.datastream.WindowedStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.WindowAssigner;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;public class window &#123; public static void main(String[] args) throws Exception &#123; //获取执行环境 StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment(); //设置并行度为2 env.setParallelism(2); //获取流数据 DataStreamSource&lt;String&gt; source = env.socketTextStream(&quot;node1&quot;, 7777); SingleOutputStreamOperator&lt;Integer&gt; map = source.map(a -&gt; Integer.parseInt(a)); //按照奇偶分组，同一组一定在同一个分区 KeyedStream&lt;Integer, Integer&gt; keyedStream = map.keyBy(a -&gt; a % 2); //设置10秒的窗口 WindowedStream&lt;Integer, Integer, TimeWindow&gt; window = keyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(10))); /** * 全窗口函数，窗口触发时才会统一触发一次 * Base abstract class for functions that are evaluated over keyed (grouped) windows using a context * for retrieving extra information. * * @param &lt;IN&gt; The type of the input value. * @param &lt;OUT&gt; The type of the output value. * @param &lt;KEY&gt; The type of the key. * @param &lt;W&gt; The type of &#123;@code Window&#125; that this window function can be applied on. */ SingleOutputStreamOperator&lt;String&gt; process = window.process(new ProcessWindowFunction&lt;Integer, String, Integer, TimeWindow&gt;() &#123; /** * * @param integer The key for which this window is evaluated. 分组的key * @param context The context in which the window is being evaluated. 上下文 * @param elements The elements in the window being evaluated. 存的数据 * @param out A collector for emitting elements. 采集器 * @throws Exception */ @Override public void process(Integer integer, ProcessWindowFunction&lt;Integer, String, Integer, TimeWindow&gt;.Context context, Iterable&lt;Integer&gt; elements, Collector&lt;String&gt; out) throws Exception &#123; long start = context.window().getStart(); long end = context.window().getEnd(); String startF = DateFormatUtils.format(start, &quot;yyyy-MM-dd HH:mm:ss.SSS&quot;); String endF = DateFormatUtils.format(end, &quot;yyyy-MM-dd HH:mm:ss.SSS&quot;); long count = elements.spliterator().estimateSize(); out.collect(&quot;key=:&quot; + integer + &quot;,窗口：&quot; + startF + &quot; &quot; + endF+&quot;,包含数据条数：&quot;+count); &#125; &#125;); process.print(); //执行 env.execute(); &#125;&#125; 增量聚合和全窗口函数结合使用所有的增量聚合函数都可以在原来的reducefuction之外，再传递一个processwindowfunction，这样数据会先经过增量聚和得到一个结果，再到全窗口函数进行封装 12345678910111213141516// ReduceFunction与WindowFunction结合public &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; reduce( ReduceFunction&lt;T&gt; reduceFunction，WindowFunction&lt;T，R，K，W&gt; function) // ReduceFunction与ProcessWindowFunction结合public &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; reduce( ReduceFunction&lt;T&gt; reduceFunction，ProcessWindowFunction&lt;T，R，K，W&gt; function)// AggregateFunction与WindowFunction结合public &lt;ACC，V，R&gt; SingleOutputStreamOperator&lt;R&gt; aggregate( AggregateFunction&lt;T，ACC，V&gt; aggFunction，WindowFunction&lt;V，R，K，W&gt; windowFunction)// AggregateFunction与ProcessWindowFunction结合public &lt;ACC，V，R&gt; SingleOutputStreamOperator&lt;R&gt; aggregate( AggregateFunction&lt;T，ACC，V&gt; aggFunction, ProcessWindowFunction&lt;V，R，K，W&gt; windowFunction) 这样调用的处理机制是： 基于第一个参数（增量聚合函数）来处理窗口数据，每来一个数据就做一次聚合 等到窗口需要触发计算时，则调用第二个参数（全窗口函数）的处理逻辑输出结果 注意：这里的全窗口函数就不再缓存所有数据了，而是直接将增量聚合函数的结果拿来当作了Iterable类型的输入。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package org.example.window;import org.apache.commons.lang3.time.DateFormatUtils;import org.apache.flink.api.common.functions.AggregateFunction;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.KeyedStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.datastream.WindowedStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.WindowAssigner;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;public class agg &#123; public static void main(String[] args) throws Exception &#123; //获取执行环境 StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment(); //设置并行度为2 env.setParallelism(2); //获取流数据 DataStreamSource&lt;String&gt; source = env.socketTextStream(&quot;node1&quot;, 7777); SingleOutputStreamOperator&lt;Integer&gt; map = source.map(a -&gt; Integer.parseInt(a)); //按照奇偶分组，同一组一定在同一个分区 KeyedStream&lt;Integer, Integer&gt; keyedStream = map.keyBy(a -&gt; a % 2); //设置10秒的窗口 WindowedStream&lt;Integer, Integer, TimeWindow&gt; window = keyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(10))); SingleOutputStreamOperator&lt;String&gt; aggregate = window.aggregate(new AggregateFunction&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer createAccumulator() &#123; System.out.println(&quot;createAcc&quot;); return 0; &#125; @Override public Integer add(Integer value, Integer accumulator) &#123; System.out.println(&quot;add&quot;); return accumulator + value; &#125; @Override public Integer getResult(Integer accumulator) &#123; System.out.println(&quot;getResult&quot;); return accumulator; &#125; @Override public Integer merge(Integer a, Integer b) &#123; return null; &#125; &#125;, new ProcessWindowFunction&lt;Integer, String, Integer, TimeWindow&gt;() &#123; @Override public void process(Integer integer, ProcessWindowFunction&lt;Integer, String, Integer, TimeWindow&gt;.Context context, Iterable&lt;Integer&gt; elements, Collector&lt;String&gt; out) throws Exception &#123; long start = context.window().getStart(); long end = context.window().getEnd(); String startF = DateFormatUtils.format(start, &quot;yyyy-MM-dd HH-mm-ss.SS&quot;); String endF = DateFormatUtils.format(end, &quot;yyyy-MM-dd HH-mm-ss.SS&quot;); long count = elements.spliterator().estimateSize(); out.collect(&quot;窗口：&quot; + startF + &quot; &quot; + endF + &quot;,key:&quot; + integer + &quot;,窗口内数据量：&quot; + count+&quot;,数据值：&quot;+elements.toString()); &#125; &#125;); aggregate.print(); //执行 env.execute(); &#125;&#125; 2.6 trigger、evictor常用的几个窗口都有默认的实现，一般不需要自定义 3. 水位线（watermark）在窗口的处理过程中，我们可以基于数据的时间戳，自定义一个“逻辑时钟”。这个时钟的时间不会自动流逝；它的时间进展，就是靠着新到数据的时间戳来推动的。 这样的好处在于，计算的过程可以完全不依赖处理时间（系统时间），不论什么时候进行统计处理，得到的结果都是正确的。而一般实时流处理的场景中，事件时间可以基本与处理时间保持同步，只是略微有一点延迟，同时保证了窗口计算的正确性。 什么是水位线在Flink中，用来衡量事件时间进展的标记，就被称作“水位线”（Watermark）。 具体实现上，水位线可以看作一条特殊的数据记录，它是插入到数据流中的一个标记点，主要内容就是一个时间戳，用来指示当前的事件时间。而它插入流中的位置，就应该是在某个数据到来之后；这样就可以从这个数据中提取时间戳，作为当前水位线的时间戳了。 水位线面临的首要问题就是数据流几乎不可能都是从小到大的有序流，乱序+迟到数据是不可避免的 总结： 水位线是插入到数据流中的一个标记，可以认为是一个特殊的数据 水位线主要的内容是一个时间戳，用来表示当前事件时间的进展 水位线是基于数据的时间戳生成的 水位线的时间戳必须单调递增，以确保任务的事件时间时钟一直向前推进 水位线可以通过设置延迟，来保证正确处理乱序数据 一个水位线Watermark(t),表示在当前流中事件时间已经达到了时间戳t,这代表t之前的所 有数据都到齐了，之后流中不会出现时间戳≤的数据 水位线是Fk流处理中保证结果正确性的核心机制，它往往会跟窗口一起配合，完成对乱序数据的正确处理 注意: 水位线是触发窗口关闭的，窗口的创建是按照到达数据的事件时间动态创建的 生成水位线在Flink的DataStream API中，有一个单独用于生成水位线的方法：.assignTimestampsAndWatermarks()，它主要用来为流中的数据分配时间戳，并生成水位线来指示事件时间。具体使用如下： 1234DataStream&lt;Event&gt; stream = env.addSource(new ClickSource());DataStream&lt;Event&gt; withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(&lt;watermark strategy&gt;); 说明：WatermarkStrategy作为参数，这就是所谓的“水位线生成策略”。WatermarkStrategy是一个接口，该接口中包含了一个“时间戳分配器”TimestampAssigner和一个“水位线生成器”WatermarkGenerator。 123456789101112public interface WatermarkStrategy&lt;T&gt; extends TimestampAssignerSupplier&lt;T&gt;, WatermarkGeneratorSupplier&lt;T&gt;&#123; // 负责从流中数据元素的某个字段中提取时间戳，并分配给元素。时间戳的分配是生成水位线的基础。 @Override TimestampAssigner&lt;T&gt; createTimestampAssigner(TimestampAssignerSupplier.Context context); // 主要负责按照既定的方式，基于时间戳生成水位线 @Override WatermarkGenerator&lt;T&gt; createWatermarkGenerator(WatermarkGeneratorSupplier.Context context);&#125; 注意：测试水位线时最好把并行度设置为1，不然思路会很乱 传入一个tuple，提取f1作为timestamp 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081package org.example.window;import org.apache.commons.lang3.time.DateFormatUtils;import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;import org.apache.flink.api.common.eventtime.WatermarkStrategy;import org.apache.flink.api.common.functions.AggregateFunction;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.KeyedStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.datastream.WindowedStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.WindowAssigner;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;public class watermark &#123; public static void main(String[] args) throws Exception &#123; //获取执行环境 StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment(); //设置并行度为1 env.setParallelism(1); //获取流数据 DataStreamSource&lt;String&gt; source = env.socketTextStream(&quot;node1&quot;, 7777); SingleOutputStreamOperator&lt;Tuple2&lt;String, String&gt;&gt; map = source.map(new MapFunction&lt;String, Tuple2&lt;String, String&gt;&gt;() &#123; @Override public Tuple2&lt;String, String&gt; map(String value) throws Exception &#123; String[] split = value.split(&quot; &quot;); return Tuple2.of(split[0],split[1]); &#125; &#125;); WatermarkStrategy&lt;Tuple2&lt;String, String&gt;&gt; ws = WatermarkStrategy //升序的watermark，没有等待时间 //需要指定tuple类型 .&lt;Tuple2&lt;String, String&gt;&gt;forMonotonousTimestamps() //指定timestamp提取逻辑 .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple2&lt;String, String&gt;&gt;() &#123; @Override public long extractTimestamp(Tuple2&lt;String, String&gt; element, long recordTimestamp) &#123; System.out.println(&quot;数据：&quot; + element + &quot;,timestamp：&quot; + recordTimestamp); return Integer.parseInt(element.f1) * 1000L; &#125; &#125;);// WatermarkStrategy&lt;Tuple2&lt;String, String&gt;&gt; ws = WatermarkStrategy.&lt;Tuple2&lt;String, String&gt;&gt;forMonotonousTimestamps()// .withTimestampAssigner((data, time) -&gt; Integer.parseInt(data.f1) * 1000L); SingleOutputStreamOperator&lt;Tuple2&lt;String, String&gt;&gt; mapWM = map.assignTimestampsAndWatermarks(ws); //这里需要使用事件时间 WindowedStream&lt;Tuple2&lt;String, String&gt;, String, TimeWindow&gt; window = mapWM.keyBy(a -&gt; a.f0) .window(TumblingEventTimeWindows.of(Time.seconds(10))); SingleOutputStreamOperator&lt;String&gt; process = window.process(new ProcessWindowFunction&lt;Tuple2&lt;String, String&gt;, String, String, TimeWindow&gt;() &#123; @Override public void process(String s, ProcessWindowFunction&lt;Tuple2&lt;String, String&gt;, String, String, TimeWindow&gt;.Context context, Iterable&lt;Tuple2&lt;String, String&gt;&gt; elements, Collector&lt;String&gt; out) throws Exception &#123; long start = context.window().getStart(); long end = context.window().getEnd(); String startF = DateFormatUtils.format(start, &quot;yyyy-MM-dd HH:MM:ss.SS&quot;); String endF = DateFormatUtils.format(end, &quot;yyyy-MM-dd HH:MM:ss.SS&quot;); long count = elements.spliterator().estimateSize(); System.out.println(&quot;触发窗口:&quot;+startF+&quot; &quot;+endF+&quot;, &quot;+elements.toString()); &#125; &#125;); //执行 env.execute(); &#125;&#125; 乱序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package org.example.window;import org.apache.commons.lang3.time.DateFormatUtils;import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;import org.apache.flink.api.common.eventtime.WatermarkStrategy;import org.apache.flink.api.common.functions.AggregateFunction;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.KeyedStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.datastream.WindowedStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.WindowAssigner;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;import org.apache.kafka.common.protocol.types.Field;import java.time.Duration;public class watermark &#123; public static void main(String[] args) throws Exception &#123; //获取执行环境 StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment(); //设置并行度为2 env.setParallelism(1); //获取流数据 DataStreamSource&lt;String&gt; source = env.socketTextStream(&quot;node1&quot;, 7777); SingleOutputStreamOperator&lt;Tuple2&lt;String, String&gt;&gt; map = source.map(new MapFunction&lt;String, Tuple2&lt;String, String&gt;&gt;() &#123; @Override public Tuple2&lt;String, String&gt; map(String value) throws Exception &#123; String[] split = value.split(&quot; &quot;); return Tuple2.of(split[0],split[1]); &#125; &#125;); //延迟时间3s WatermarkStrategy&lt;Tuple2&lt;String, String&gt;&gt; ws = WatermarkStrategy.&lt;Tuple2&lt;String, String&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(3)) .withTimestampAssigner((data, time) -&gt; Integer.parseInt(data.f1) * 1000L); SingleOutputStreamOperator&lt;Tuple2&lt;String, String&gt;&gt; mapWM = map.assignTimestampsAndWatermarks(ws); //这里需要使用事件时间 WindowedStream&lt;Tuple2&lt;String, String&gt;, String, TimeWindow&gt; window = mapWM.keyBy(a -&gt; a.f0) .window(TumblingEventTimeWindows.of(Time.seconds(10))); SingleOutputStreamOperator&lt;String&gt; process = window.process(new ProcessWindowFunction&lt;Tuple2&lt;String, String&gt;, String, String, TimeWindow&gt;() &#123; @Override public void process(String s, ProcessWindowFunction&lt;Tuple2&lt;String, String&gt;, String, String, TimeWindow&gt;.Context context, Iterable&lt;Tuple2&lt;String, String&gt;&gt; elements, Collector&lt;String&gt; out) throws Exception &#123; long start = context.window().getStart(); long end = context.window().getEnd(); String startF = DateFormatUtils.format(start, &quot;yyyy-MM-dd HH:MM:ss.SS&quot;); String endF = DateFormatUtils.format(end, &quot;yyyy-MM-dd HH:MM:ss.SS&quot;); long count = elements.spliterator().estimateSize(); System.out.println(&quot;触发窗口:&quot;+startF+&quot; &quot;+endF+&quot;, &quot;+elements.toString()); &#125; &#125;); //执行 env.execute(); &#125;&#125; 水位线的传递 在流处理中，上游任务处理完水位线、时钟改变之后，要把当前的水位线再次发出，广播给所有的下游子任务。而当一个任务接收到多个上游并行任务传递来的水位线时，应该以最小的那个作为当前任务的事件时钟。 当然这也带来一个问题。 在多个上游并行任务中，如果有其中一个没有数据，由于当前Task是以最小的那个作为当前任务的事件时钟，就会导致当前Task的水位线无法推进，就可能导致窗口无法触发。这时候可以设置空闲等待。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package org.example.window;import org.apache.commons.lang3.time.DateFormatUtils;import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;import org.apache.flink.api.common.eventtime.WatermarkStrategy;import org.apache.flink.api.common.functions.AggregateFunction;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.common.functions.Partitioner;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.KeyedStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.datastream.WindowedStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.WindowAssigner;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;import org.apache.kafka.common.protocol.types.Field;import java.time.Duration;public class watermark &#123; public static void main(String[] args) throws Exception &#123; //获取执行环境 StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment(); //设置并行度为2 env.setParallelism(2); //获取流数据 DataStreamSource&lt;String&gt; source = env.socketTextStream(&quot;node1&quot;, 7777); //分区，让数据按照奇偶分别进入0和1分区 //这样只输入奇数，则map只有一个分区是有数据的，另一个分区一直是空，这样算子从map获取的水位线一直是最小值，不会触发窗口 SingleOutputStreamOperator&lt;Integer&gt; map = source.partitionCustom(new Partitioner&lt;String&gt;() &#123; @Override public int partition(String s, int numPartitions) &#123; System.out.println(&quot;当前数字：&quot;+s+&quot;,分区数&quot;+numPartitions+&quot;，进入分区：&quot;+Integer.parseInt(s)%2); return Integer.parseInt(s) % 2; &#125; &#125;,a-&gt;a).map(a -&gt; Integer.parseInt(a)); //水位线生成策略 WatermarkStrategy&lt;Integer&gt; ws = WatermarkStrategy.&lt;Integer&gt;forBoundedOutOfOrderness(Duration.ofSeconds(3)) .withTimestampAssigner((a, ts) -&gt; a * 1000L) //如不设置空闲等待，窗口一直不会触发 .withIdleness(Duration.ofSeconds(5));//空闲等待五秒 SingleOutputStreamOperator&lt;Integer&gt; watermarks = map.assignTimestampsAndWatermarks(ws); //这里需要使用事件时间 WindowedStream&lt;Integer, Integer, TimeWindow&gt; window = watermarks.keyBy(a -&gt; a % 2).window(TumblingEventTimeWindows.of(Time.seconds(10))); SingleOutputStreamOperator&lt;Integer&gt; wd = window.process(new ProcessWindowFunction&lt;Integer, Integer, Integer, TimeWindow&gt;() &#123; @Override public void process(Integer integer, ProcessWindowFunction&lt;Integer, Integer, Integer, TimeWindow&gt;.Context context, Iterable&lt;Integer&gt; elements, Collector&lt;Integer&gt; out) throws Exception &#123; System.out.println(&quot;触发窗口&quot;); &#125; &#125;); //执行 env.execute(); &#125;&#125; 如不设置空闲等待，则运行如下图 1234WatermarkStrategy&lt;Integer&gt; ws = WatermarkStrategy.&lt;Integer&gt;forBoundedOutOfOrderness(Duration.ofSeconds(3)) .withTimestampAssigner((a, ts) -&gt; a * 1000L) //如不设置空闲等待，窗口一直不会触发 //.withIdleness(Duration.ofSeconds(5));//空闲等待五秒 迟到数据的处理推迟水印推进在水印产生时，设置一个乱序容忍度，推迟系统时间的推进，保证窗口计算被延迟执行，为乱序的数据争取更多的时间进入窗口。 WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(10)); 设置窗口延迟关闭 Flink的窗口，也允许迟到数据。当触发了窗口计算后，会先计算当前的结果，但是此时并不会关闭窗口。 以后每来一条迟到数据，就触发一次这条数据所在窗口计算(增量计算)。直到wartermark 超过了窗口结束时间+推迟时间，此时窗口会真正关闭。 12.window(TumblingEventTimeWindows.of(Time.seconds(5))).allowedLateness(Time.seconds(3)) 注意: 允许迟到只能运用在event time上 使用侧流接收迟到的数据双流连接窗口联结（Window Join）12345stream1.join(stream2) .where(&lt;KeySelector&gt;) .equalTo(&lt;KeySelector&gt;) .window(&lt;WindowAssigner&gt;) .apply(&lt;JoinFunction&gt;) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package org.example.window;import org.apache.flink.api.common.eventtime.WatermarkStrategy;import org.apache.flink.api.common.functions.JoinFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.tuple.Tuple3;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;public class windowjoin &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;String&gt; source = env.socketTextStream(&quot;node1&quot;, 7777); env.setParallelism(1); SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; ds1 = env .fromElements( Tuple2.of(&quot;a&quot;, 1), Tuple2.of(&quot;a&quot;, 2), Tuple2.of(&quot;b&quot;, 3), Tuple2.of(&quot;c&quot;, 4) ) .assignTimestampsAndWatermarks( WatermarkStrategy .&lt;Tuple2&lt;String, Integer&gt;&gt;forMonotonousTimestamps() .withTimestampAssigner((value, ts) -&gt; value.f1 * 1000L) ); SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer,Integer&gt;&gt; ds2 = env .fromElements( Tuple3.of(&quot;a&quot;, 1,1), Tuple3.of(&quot;a&quot;, 11,1), Tuple3.of(&quot;b&quot;, 2,1), Tuple3.of(&quot;b&quot;, 12,1), Tuple3.of(&quot;c&quot;, 14,1), Tuple3.of(&quot;d&quot;, 15,1) ) .assignTimestampsAndWatermarks( WatermarkStrategy .&lt;Tuple3&lt;String, Integer,Integer&gt;&gt;forMonotonousTimestamps() .withTimestampAssigner((value, ts) -&gt; value.f1 * 1000L) ); DataStream&lt;String&gt; apply = ds1.join(ds2).where(a -&gt; a.f0).equalTo(b -&gt; b.f0).window(TumblingEventTimeWindows.of(Time.seconds(5))).apply(new JoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;() &#123; @Override public String join(Tuple2&lt;String, Integer&gt; first, Tuple3&lt;String, Integer, Integer&gt; second) throws Exception &#123; return first + &quot;====&quot; + second; &#125; &#125;); apply.print(); env.execute(); &#125;&#125; 间隔联结（Interval Join）窗口连接是双方都在同一个窗口内才会发生链接，但是如果出现a在9s，b在11s，两条数据就匹配不上了，显然不符合现实要求，所以更推荐使用间隔联结 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package org.example.window;import org.apache.flink.api.common.eventtime.WatermarkStrategy;import org.apache.flink.api.common.functions.JoinFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.tuple.Tuple3;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.KeyedStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.co.ProcessJoinFunction;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.util.Collector;public class windowjoin &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;String&gt; source = env.socketTextStream(&quot;node1&quot;, 7777); env.setParallelism(1); SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; ds1 = env .fromElements( Tuple2.of(&quot;a&quot;, 1), Tuple2.of(&quot;a&quot;, 2), Tuple2.of(&quot;b&quot;, 3), Tuple2.of(&quot;c&quot;, 4) ) .assignTimestampsAndWatermarks( WatermarkStrategy .&lt;Tuple2&lt;String, Integer&gt;&gt;forMonotonousTimestamps() .withTimestampAssigner((value, ts) -&gt; value.f1 * 1000L) ); SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer,Integer&gt;&gt; ds2 = env .fromElements( Tuple3.of(&quot;a&quot;, 1,1), Tuple3.of(&quot;a&quot;, 11,1), Tuple3.of(&quot;b&quot;, 2,1), Tuple3.of(&quot;b&quot;, 12,1), Tuple3.of(&quot;c&quot;, 14,1), Tuple3.of(&quot;d&quot;, 15,1) ) .assignTimestampsAndWatermarks( WatermarkStrategy .&lt;Tuple3&lt;String, Integer,Integer&gt;&gt;forMonotonousTimestamps() .withTimestampAssigner((value, ts) -&gt; value.f1 * 1000L) ); KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; d1 = ds1.keyBy(a -&gt; a.f0); KeyedStream&lt;Tuple3&lt;String, Integer, Integer&gt;, String&gt; d2 = ds2.keyBy(a -&gt; a.f0); SingleOutputStreamOperator&lt;String&gt; process = d1.intervalJoin(d2).between(Time.seconds(-2), Time.seconds(2)) .process(new ProcessJoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;() &#123; /** * 两条流数据匹配上才会触发 * @param left The left element of the joined pair. * @param right The right element of the joined pair. * @param ctx A context that allows querying the timestamps of the left, right and joined pair. * In addition, this context allows to emit elements on a side output. * @param out The collector to emit resulting elements to. * @throws Exception */ @Override public void processElement(Tuple2&lt;String, Integer&gt; left, Tuple3&lt;String, Integer, Integer&gt; right, ProcessJoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;.Context ctx, Collector&lt;String&gt; out) throws Exception &#123; System.out.println(left + &quot;====&quot; + right); &#125; &#125;); process.print(); env.execute(); &#125;&#125; 迟到数据的处理：outputTag 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package org.example.window;import org.apache.flink.api.common.eventtime.WatermarkStrategy;import org.apache.flink.api.common.functions.JoinFunction;import org.apache.flink.api.common.typeinfo.Types;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.tuple.Tuple3;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.KeyedStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.co.ProcessJoinFunction;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.util.Collector;import org.apache.flink.util.OutputTag;import java.time.Duration;public class windowjoin &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; ds1 = env.socketTextStream(&quot;node1&quot;,7777).map(a-&gt;&#123; String[] split = a.split(&quot; &quot;); return Tuple2.of(split[0],Integer.parseInt(split[1])); &#125;).returns(Types.TUPLE(Types.STRING,Types.INT)) .assignTimestampsAndWatermarks( WatermarkStrategy. &lt;Tuple2&lt;String,Integer&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(3)) .withTimestampAssigner((a,r)-&gt;a.f1*1000L) ); SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer,Integer&gt;&gt; ds2 = env .socketTextStream(&quot;node1&quot;,8888).map(a-&gt;&#123; String[] split = a.split(&quot; &quot;); return Tuple3.of(split[0],Integer.parseInt(split[1]),Integer.parseInt(split[2])); &#125;).returns(Types.TUPLE(Types.STRING,Types.INT,Types.INT)) .assignTimestampsAndWatermarks( WatermarkStrategy .&lt;Tuple3&lt;String, Integer,Integer&gt;&gt;forMonotonousTimestamps() .withTimestampAssigner((value, ts) -&gt; value.f1 * 1000L) ); KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; d1 = ds1.keyBy(a -&gt; a.f0); KeyedStream&lt;Tuple3&lt;String, Integer, Integer&gt;, String&gt; d2 = ds2.keyBy(a -&gt; a.f0); OutputTag&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; rightLate = new OutputTag&lt;&gt;(&quot;rightLate&quot;, Types.TUPLE(Types.STRING, Types.INT, Types.INT)); OutputTag&lt;Tuple2&lt;String, Integer&gt;&gt; leftLate = new OutputTag&lt;&gt;(&quot;leftLate&quot;, Types.TUPLE(Types.STRING, Types.INT)); SingleOutputStreamOperator&lt;String&gt; process = d1.intervalJoin(d2).between(Time.seconds(-2), Time.seconds(2)) .sideOutputRightLateData(rightLate) .sideOutputLeftLateData(leftLate) .process(new ProcessJoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;() &#123; /** * 两条流数据匹配上才会触发 * @param left The left element of the joined pair. * @param right The right element of the joined pair. * @param ctx A context that allows querying the timestamps of the left, right and joined pair. * In addition, this context allows to emit elements on a side output. * @param out The collector to emit resulting elements to. * @throws Exception */ @Override public void processElement(Tuple2&lt;String, Integer&gt; left, Tuple3&lt;String, Integer, Integer&gt; right, ProcessJoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;.Context ctx, Collector&lt;String&gt; out) throws Exception &#123; out.collect(left + &quot;====&quot; + right); &#125; &#125;); process.print(&quot;主流&quot;); process.getSideOutput(rightLate).printToErr(&quot;rightlate&quot;); process.getSideOutput(leftLate).printToErr(&quot;leftlate&quot;); env.execute(); &#125;&#125;","categories":["大数据","Flink"]},{"title":"Flink(1)——入门：wordcount","path":"/2024/02/17/Flink(1)——入门：wordcount/","content":"简介关于Flink的一些核心概念这篇文章已经说的很清楚了，就不再赘述。 Flink-数据流编程模型-阿里云开发者社区 (aliyun.com) 个人还是秉持着边做边想、拒绝空转的理念，先简单的写一两个程序，然后回头看概念会更好理解。 本文会从wordcount开始，简单的过一遍Flink的DataStream API。 一、wordcount这里以最简单的wordcount为例，先把程序跑起来 先引入maven依赖 12345678910111213141516171819202122232425262728293031323334&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;flink_01&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;flink.version&gt;1.17.0&lt;/flink.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 具体代码如下 1234567891011121314151617181920212223242526272829303132333435package org.example;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.util.Collector;public class Main &#123; public static void main(String[] args) throws Exception &#123; //获取执行环境 StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment(); //设置并行度为1 env.setParallelism(1); //从wsl的7777端口监听数据 DataStreamSource&lt;String&gt; lineStream=env.socketTextStream(&quot;wsl&quot;,7777); //转换、分组、求和 SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = lineStream.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) throws Exception &#123; String[] split = value.split(&quot; &quot;); for (String s : split) &#123; out.collect(Tuple2.of(s, 1)); &#125; &#125; &#125;).keyBy(data -&gt; data.f0).sum(1); //打印 sum.print(); //触发执行 env.execute(); &#125;&#125; 输入输出如下 二、详解wordcountFlink提供了不同等级的抽象来开发流&#x2F;批处理程序 DataStream API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。 具体来说，代码基本上都由以下几部分构成： Flink程序的基本构建模块是streams 和 transformations 。 从概念上讲，streams 是数据记录的(可能是无限的)流，而transformations是将一个或多个流作为输入并产生一个或多个输出流的操作。 执行时，Flink程序被映射到流数据流，由streams 和 transformations 操作符组成。每个数据流以一个或多个sources开始，以一个或多个sinks结束。数据流类似于任意有向无环图(DAGs)。 具体到countword程序 1234567891011121314151617181920212223242526public class Main &#123; public static void main(String[] args) throws Exception &#123; //1.获取执行环境 StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment(); //设置并行度为1 env.setParallelism(1); //2.读取数据源：从wsl的7777端口监听数据 DataStreamSource&lt;String&gt; lineStream=env.socketTextStream(&quot;wsl&quot;,7777); //3.transformation:转换、分组、求和 SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = lineStream.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) throws Exception &#123; String[] split = value.split(&quot; &quot;); for (String s : split) &#123; out.collect(Tuple2.of(s, 1)); &#125; &#125; &#125;).keyBy(data -&gt; data.f0).sum(1); //4.sink:打印 sum.print(); //触发执行 env.execute(); &#125;&#125; print()的底层就是sink 可以看到，Flink程序是严格按照四步流程进行的 三、Flink算子和程序流程对应，Flink算子也可以分成四类 执行环境（execution environment） 源算子（source） 转换算子（transformation） 输出算子（sink） 转换算子简单转换算子（map&#x2F;filter&#x2F;flatmap）123456789101112131415161718192021222324252627282930313233343536373839404142434445package org.example;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.util.Collector;public class Main &#123; public static void main(String[] args) throws Exception &#123; //获取执行环境 StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment(); //设置并行度为1 env.setParallelism(1); //从wsl的7777端口监听数据 DataStreamSource&lt;String&gt; lineStream=env.socketTextStream(&quot;wsl&quot;,7777); //map实现1对1的转换，消费一个元素就产出一个元素 //SingleOutputStreamOperator&lt;Integer&gt; map = lineStream.map(a -&gt; Integer.parseInt(a) * 10); //map.print(); //filter对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。 //SingleOutputStreamOperator&lt;String&gt; filter = lineStream.filter(a -&gt; Integer.parseInt(a) % 2 == 0); //filter.print(); //flat主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。 //flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。 SingleOutputStreamOperator&lt;Integer&gt; flatmapValue = lineStream.flatMap(new FlatMapFunction&lt;String, Integer&gt;() &#123; @Override public void flatMap(String value, Collector&lt;Integer&gt; out) throws Exception &#123; if (Integer.parseInt(value) % 2 == 0) &#123; out.collect(Integer.parseInt(value)); &#125; else &#123; out.collect(Integer.parseInt(value)); out.collect(Integer.parseInt(value) * 10); &#125; &#125; &#125;); flatmapValue.print(); env.execute(); &#125;&#125; 聚合算子除了简单转换算子，聚合算子也是必不可少的。 对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。 所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。 keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。（此前一直设置为1） 基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。 ！注意：key值相同的数据一定在同一分区，但是同一分区可以包含多个不同的key值 在内部，是通过计算key的哈希值（hash code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode()方法。 ……","categories":["大数据","Flink"]},{"title":"MapReduce——join","path":"/2024/02/17/MapReduce——join/","content":"一、背景知识在实际的数据库应用中，我们经常需要从多个数据表中读取数据，这时我们就可以使用SQL语句中的连接（JOIN），在两个或多个数据表中查询数据。 在使用MapReduce框架进行数据处理的过程中，也会涉及到从多个数据集读取数据，进行join关联的操作，只不过此时需要使用java代码并且根据MapReduce的编程规范进行业务的实现。 但是由于MapReduce的分布式设计理念的特殊性，因此对于MapReduce实现join操作具备了一定的特殊性。特殊主要体现在：究竟在MapReduce中的什么阶段进行数据集的关联操作，是mapper阶段还是reducer阶段，之间的区别又是什么？ 整个MapReduce的join分为两类：map side join、reduce side join。 二、reduce side join原理解析回顾整个mapreduce过程，其实join操作是非常好实现的，因为&lt;k,v&gt;键值对+shuffle天然就会把key值一样的数据放到同一个分区进行reduce。 这样的话map阶段输入&lt;行偏移量，本行内容&gt;，输出&lt;连接关键词,本行内容&gt;，然后在reduce阶段对两个文件做笛卡尔积即可，当然这里需要额外对数据打上标记，注明是哪一个文件 总结流程如下 mapper分别读取不同的数据集； mapper的输出中，以join的字段作为输出的key； 不同数据集的数据经过shuffle，key一样的会被分到同一分组处理； 在reduce中根据业务需求把数据进行关联整合汇总，最终输出。 代码详解12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package org.example;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapred.FileSplit;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;/** * k1:LongWritable 行偏移量 * v1:Text 行数据 * k2:Text 商品id * v2:Text 行数据 */public class ReduceJoinMapper extends Mapper&lt;LongWritable, Text,Text,Text&gt; &#123; //设置outKey和outValue Text outKey=new Text(); Text outValue=new Text(); StringBuilder sb=new StringBuilder(); String filename=null; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; //1:判断文件来自哪个文件 FileSplit fileSplit = (FileSplit) context.getInputSplit(); //获取文件名称 filename = fileSplit.getPath().getName(); if (&quot;goods.txt&quot;.equals(filename))&#123; //商品id，商品编号，商品名称 String[] words = value.toString().split(&quot;[|]&quot;); String id=words[0]; sb.append(&quot;goods#&quot;).append(words[1]).append(&quot;\\t&quot;).append(words[2]); outKey.set(id); outValue.set(sb.toString()); context.write(outKey,outValue); &#125;else &#123; //订单编号，商品id，付款金额 String[] words = value.toString().split(&quot;[|]&quot;); String id=words[1]; sb.append(&quot;order#&quot;).append(words[0]).append(&quot;\\t&quot;).append(words[2]); outKey.set(id); outValue.set(sb.toString()); context.write(outKey,outValue); &#125; //2：转化k2,v2写入上下文 super.map(key, value, context); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940package org.example;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;import java.util.LinkedList;import java.util.List;/** * */public class ReduceJoinReducer extends Reducer&lt;Text,Text,Text,Text&gt; &#123; //这里outkey就是key(goods_id)，不需要更新 //Text outKey=new Text(); Text outValue=new Text(); List&lt;String&gt;goods=new LinkedList&lt;&gt;(); List&lt;String&gt;orders=new LinkedList&lt;&gt;(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Reducer&lt;Text, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; for (Text value : values) &#123; if (value.toString().startsWith(&quot;goods#&quot;))&#123; String s = value.toString().split(&quot;#&quot;)[1]; goods.add(s); &#125;else &#123; String s = value.toString().split(&quot;#&quot;)[1]; orders.add(s); &#125; &#125; //2.将k3和v3写入context int goodSize=goods.size(); int orderSize= orders.size(); for (int i = 0; i &lt; goodSize; i++) &#123; for (int j = 0; j &lt; orderSize; j++) &#123; outValue.set(goods.get(i)+&quot;\\t&quot;+orders.get(j)); context.write(key,outValue); &#125; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package org.example;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class JobMain extends Configured implements Tool &#123; @Override public int run(String[] strings) throws Exception &#123; /** * 1.获取job对象 * 2.设置job任务 * 3.等待job任务结束 */ // 1.获取job对象 Job job = Job.getInstance(super.getConf(), &quot;reduce_join&quot;); //2.设置job任务,8步 //2.1设置输入类和输入路径 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job,new Path(&quot;hdfs://node1:8020:/input/reduce_join&quot;)); //2.2设置Mapper类和数据类型 job.setMapperClass(ReduceJoinMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); //3,4,5,6略 //2.7设置reducer类和数据类型 job.setReducerClass(ReduceJoinReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); //2.8设置输出类和输出路径 job.setOutputFormatClass(TextOutputFormat.class); TextOutputFormat.setOutputPath(job,new Path(&quot;hdfs://node1:8020:/output/reduce_join_out&quot;)); boolean bl=job.waitForCompletion(true); return bl?0:1; &#125; public static void main(String[] args) throws Exception &#123; //启动job任务 Configuration configuration = new Configuration(); int run=ToolRunner.run(configuration,new JobMain(),args); System.exit(run); &#125;&#125; 问题reduce端join最大的问题是整个join的工作是在reduce阶段完成的，但是通常情况下MapReduce中reduce的并行度是极小的（默认是1个），这就使得所有的数据都挤压到reduce阶段处理，压力颇大。虽然可以设置reduce的并行度，但是又会导致最终结果被分散到多个不同文件中。并且在数据从mapper到reducer的过程中，shuffle阶段十分繁琐，数据集大时成本极高。 三、map side join分布式缓存为了解决reduce join的问题，可以引入分布式缓存。 DistributedCache是hadoop框架提供的一种机制,可以将job指定的文件,在job执行前,先行分发到task执行的机器上,并有相关机制对cache文件进行管理（和spark的广播非常像，当然最初学的时候还不知道什么是spark）。 DistributedCache能够缓存应用程序所需的文件 （包括文本，档案文件，jar文件等）。 Map-Redcue框架在作业所有任务执行之前会把必要的文件拷贝到slave节点上。 它运行高效是因为每个作业的文件只拷贝一次并且为那些没有文档的slave节点缓存文档。 原理解析map side join的大致思路如下： 首先分析join处理的数据集，使用分布式缓存技术将小的数据集进行分布式缓存 MapReduce框架在执行的时候会自动将缓存的数据分发到各个maptask运行的机器上 程序只运行mapper，在mapper初始化的时候从分布式缓存中读取小数据集数据，然后和自己读取的大数据集进行join关联，输出最终的结果。 整个join的过程没有shuffle，没有reducer。 代码详解1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package cn.itcast.hadoop.mapreduce.join.mapside;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.util.HashMap;import java.util.Map;/** * @description: */public class MapJoinMapper extends Mapper&lt;LongWritable, Text,Text, NullWritable&gt; &#123; //创建集合 用于缓存商品数据goods.txt Map&lt;String, String&gt; goodsMap = new HashMap&lt;String,String&gt;(); Text k = new Text(); /** * 在程序的初始化方法中 从分布式缓存中加载缓存文件 写入goodsMap集合中 */ @Override protected void setup(Context context) throws IOException, InterruptedException &#123; //读取缓存文件 BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(&quot;goods.txt&quot;))); String line = null; while((line=br.readLine())!=null)&#123; //一行数据格式为: 100101|155083444927602|四川果冻橙6个约180g（商品id，商品编号，商品名称） String[] fields = line.split(&quot;\\\\|&quot;); goodsMap.put(fields[0], fields[1]+&quot;\\t&quot;+fields[2]); &#125; &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //一行订单数据 格式为： 1|107860|7191 (订单编号，商品id,实际支付价格) String[] fields = value.toString().split(&quot;\\\\|&quot;); //根据订单数据中商品id在缓存中找出来对应商品信息(商品名称)，进行串接 String goodsInfo = goodsMap.get(fields[1]); k.set(value.toString()+&quot;\\t&quot;+goodsInfo); //所有数据都保存在key中 context.write(k, NullWritable.get()); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package cn.itcast.hadoop.mapreduce.join.mapside;import cn.itcast.hadoop.mapreduce.join.reduceside.ReduceJoinSortApp;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.net.URI;/** * @description: */public class MapJoinDriver &#123; public static void main(String[] args) throws Exception, InterruptedException &#123; Configuration conf = new Configuration(); // 创建作业实例 Job job = Job.getInstance(conf, MapJoinDriver.class.getSimpleName()); // 设置作业驱动类 job.setJarByClass(MapJoinDriver.class); // 设置作业mapper job.setMapperClass(MapJoinMapper.class); // 设置作业mapper阶段输出key value数据类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); //设置作业最终输出数据类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); //todo 添加分布式缓存文件 job.addCacheFile(new URI(&quot;/data/join/cache/goods.txt&quot;)); //不需要reduce，那么也就没有了shuffle过程 job.setNumReduceTasks(0); // 配置作业的输入数据路径 FileInputFormat.addInputPath(job, new Path(&quot;/data/join/input&quot;)); // 配置作业的输出数据路径 FileOutputFormat.setOutputPath(job, new Path(&quot;/data/join/mrresult&quot;)); // 提交作业并等待执行完成 boolean b = job.waitForCompletion(true); System.exit(b ? 0 :1); &#125;&#125;","categories":["大数据","MapReduce"]},{"title":"MapReduce——countword","path":"/2024/02/16/MapReduce——countword/","content":"吐槽代码一定得要自己写一遍，对于mapreduce，mentor安排的学习目标并不困难，了解思想学习原理，知道怎么编写mapreduce程序，能够实现一个join操作，但是真上手还是小坑不断，就说wordcount wordcount绝对算是简单的不能再简单了，不存在任何编程和理解上的难度，但是万万没想到，nativeio竟然会出问题 1Exception in thread &quot;main&quot; java.lang.UnsatisfiedLinkError: &#x27;org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(java.lang.String)&#x27; 反复排查之后发现是windows下的input必须要通过通配符的形式表示，不能像linux一样用文件夹表示 1234FileInputFormat.addInputPath(job, new Path(&quot;D:\\\\bigdata\\\\Java_demo\\\\wordcount\\\\*.txt&quot;));//省略通配符则会报如上错误，令人抓狂FileInputFormat.addInputPath(job, new Path(&quot;D:\\\\bigdata\\\\Java_demo\\\\wordcount&quot;)); 查了很多博客，有说需要改版本依赖的，有说要改org.apache.hadoop.io.nativeio源码的，乱七八糟，最终通过通配符的方式解决，让人哭笑不得。 按道理这个问题应该非常常见，每个初学者都会遇到，但是相关博客竟然寥寥无几，大家测试都不用本地的？ 正文如下 一、思想概述: Map&amp;ReduceMapReduce思想在生活中处处可见，核心就是“先分再合，分而治之” 所谓“分而治之”就是把一个复杂的问题，按照一定的“分解”方法分为等价的规模较小的若干部分，然后逐个解决，分别找出各部分的结果，把各部分的结果组成整个问题的结果。 这种思想来源于日常生活与工作时的经验，同样也完全适用于大量复杂的任务处理场景（大规模数据处理场景） 一个简单的例子，10排队伍，每排一个队长负责清点本排队伍的人数，这就是map，这10个人会合把各排人数汇总到到一起就是reduce。 Map负责“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。 Reduce负责“合”，即对map阶段的结果进行全局汇总。 这两个阶段合起来正是MapReduce 二、 架构体系一个完整的mapreduce程序在分布式运行时有三类实例进程： MRAppMaster：负责整个程序的过程调度及状态协调 MapTask：负责map阶段的整个数据处理流程 ReduceTask：负责reduce阶段的整个数据处理流程 三、 编程规范MapReduce分布式的运算程序需要分成2个阶段，分别是Map阶段和Reduce阶段。 Map阶段对应的是MapTask并发实例，完全并行运行。Reduce阶段对应的是ReduceTask并发实例，数据依赖于上一个阶段所有MapTask并发实例的数据输出结果。 MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端驱动)。 用户自定义的Mapper和Reducer都要继承各自的父类。Mapper中的业务逻辑写在map()方法中，Reducer的业务逻辑写在reduce()方法中。整个程序需要一个Driver来进行提交，提交的是一个描述了各种必要信息的job对象。 四 、 MapReduce工作执行流程整个MapReduce工作流程可以分为3个阶段：map、shuffle、reduce。 map阶段：负责把从数据源读取来到数据进行处理，默认情况下读取数据返回的是kv键值对类型，经过自定义map方法处理之后，输出的也应该是kv键值对类型。 shuffle阶段：map输出的数据会经过分区、排序、分组等自带动作进行重组，相当于洗牌的逆过程。这是MapReduce的核心所在，也是难点所在。默认分区规则：key相同的分在同一个分区，同一个分区被同一个reduce处理。默认排序规则：根据key字典序排序默认分组规则：key相同的分为一组，一组调用reduce处理一次。 reduce阶段：负责针对shuffle好的数据进行聚合处理。输出的结果也应该是kv键值对。 五、数据类型hadoop的序列化没有采用java的序列化机制，而是实现了自己的序列化机制。 hadoop提供了如下内容的数据类型，这些数据类型都实现了WritableComparable接口，以便用这些类型定义的数据可以被序列化进行网络传输和文件存储，以及进行大小比较。 Hadoop 数据类型 Java 数据类型 备注 BooleanWritable boolean 标准布尔型数值 ByteWritable byte 单字节数值 IntWritable int 整型数 FloatWritable float 浮点数 LongWritable long 长整型数 DoubleWritable double 双字节数值 Text String 使用UTF8格式存储的文本 MapWritable map 映射 ArrayWritable array 数组 NullWritable null 当&lt;key,value&gt;中的key或value为空时使用 六、wordcountWordCount中文叫做单词统计、词频统计，指的是使用程序统计某文本文件中，每个单词出现的总次数。这个是大数据计算领域经典的入门案例 代码与注释如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114package org.example;import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.http.util.Args;public class WordCount &#123; /** * Mapper泛型： * KEYIN：k1的类型，行偏移量 * VALUEIN：v1的类型，一行的文本数据 * KEYOUT：k2的类型，每个单词 * VALUEOUT：v2的类型，固定值1 */ public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123; private final static IntWritable one = new IntWritable(1); private Text word = new Text(); /** * * map方法将k1和v1转化成k2和v2 * @param key k1 * @param value v1 * @param context mapreduce上下文对象，是连接map和reduce之间的桥梁 */ public void map(Object key, Text value, Context context ) throws IOException, InterruptedException &#123; //拿到一行数据 //StringTokenizer类：根据自定义字符为分界符进行拆分，并将结果进行封装提供对应方法进行遍历取值 //java默认的分隔符是“空格”、“制表符(‘\\t’)”、“换行符(‘ ’)”、“回车符(‘\\r’)” //StringTokenizer默认的话，所有的分隔符都会同时起作用 //也因此 hello,world 对StringTokenizer是一个字符， //StringTokenizer itr = new StringTokenizer(value.toString()); String[] itr = value.toString().split(&quot;,&quot;); for (String s:itr) &#123; word.set(s); //调用context.wirte()将k2和v2向后传 context.write(word, one); &#125; &#125; &#125; /** * Reducer四个泛型： * KEYIN：k2的类型，每个单词 * VALUEIN：v2的类型，集合中泛型的类型 * KEYOUT：k3的类型，每个单词 * VALUEOUT：v3的类型，每个单词的数量 */ public static class IntSumReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123; private IntWritable result = new IntWritable(); /** * reduce方法将k2 v2转化成k3和v3 * @param key k2 * @param values 集合 * @param context 上下文对象 */ public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException &#123; //遍历集合，相加 int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; result.set(sum); //k3和v3写入上下文 context.write(key, result); &#125; &#125; public static void main(String[] args) throws Exception &#123; //创建任务对象 Configuration conf = new Configuration(); conf.set(&quot;mapreduce.framework.name&quot;,&quot;local&quot;); Job job = Job.getInstance(conf, &quot;word count&quot;); //TextInputFormat读取的k1和v1是行偏移量和行数据 //job.setInputFormatClass(TextInputFormat.class); //TextInputFormat.addInputPath(job,new Path(&quot;hdfs://node1:8020/input/wordcount&quot;)); job.setJarByClass(WordCount.class); //指定map阶段处理方法 job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //指定输入输出路径 FileInputFormat.addInputPath(job, new Path(&quot;D:\\\\bigdata\\\\Java_demo\\\\wordcount\\\\*.txt&quot;)); FileOutputFormat.setOutputPath(job, new Path(&quot;D:\\\\bigdata\\\\Java_demo\\\\wordcount_output&quot;)); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 流程如下 第一阶段是把输入目录下文件按照一定的标准逐个进行逻辑切片，形成切片规划。默认情况下，Split size &#x3D; Block size。每一个切片由一个MapTask处理。（getSplits） 第二阶段是对切片中的数据按照一定的规则解析成&lt;key,value&gt;对。默认规则是把每一行文本内容解析成键值对。key是每一行的起始位置(单位是字节)，value是本行的文本内容。（TextInputFormat） 第三阶段是调用Mapper类中的map方法。上阶段中每解析出来的一个&lt;k,v&gt;，调用一次map方法。每次调用map方法会输出零个或多个键值对。 第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。默认是只有一个区。分区的数量就是Reducer任务运行的数量。默认只有一个Reducer任务。 第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有第六阶段，那么进入第六阶段；如果没有，直接输出到文件中。 第六阶段是对数据进行局部聚合处理，也就是combiner处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。本阶段默认是没有的。","categories":["大数据","MapReduce"]},{"title":"从零开始搭建hadoop集群","path":"/2024/02/16/从零开始搭建hadoop集群/","content":"目标从零开始搭建Hadoop集群，包括HDFS、YARN集群，目标是可以在本集群上成功运行官方提供的mapreduce_example任务 虚拟机配置（基于VMware）准备好三台Linux虚拟机环境，实现相互之间的SSH免密登录 首先在VMware里创建一台CentOS虚拟机（本文基于CentOS 7.6版本）作为基础虚拟机 VMware下载地址 CentOS下载地址 一路默认下一步即可 通过克隆得到另外两台虚拟机，建立centos_Hadoop集群文件夹方便管理 修改主机名以及hosts文件，方便后续远程连接（可选，但强烈建议） 注意：配置阶段均在root用户下修改，最终集群会在hadoop用户下运行 以node1主机为例，通过ifconfig命令查看ip地址 当前主机的ip地址为192.168.88.101，如不配置hosts文件，则在后续修改配置文件时，或者每次ssh时，都要输入完整的ip地址，但是修改&#x2F;etc&#x2F;hosts文件之后，可以直接通过host name访问，后续操作用到ip地址的地方很多，前期配置好host name之后会轻松很多 what is the hosts file The Hosts file (also referred to as etc&#x2F;hosts) is a text file used by Windows (and other operating systems) to map IP addresses to host names or domain names. This file acts as a local DNS service, for your local computer, and it overrides the mappings from the DNS server that your computer is connected to, through the network. 1vim /etc/hosts 同样可以在windows中修改hosts文件，这样后续访问如hdfs的web管理页面时可以直接通过host name访问 关闭防火墙 有些系统如Ubuntu可能未安装防火墙，则此步骤可跳过 12345678# 关闭防火墙systemctl stop firewalldsystemctl disable firewalld#关闭SELinuxvim /etc/sysconfig/selinux# 将SELINUX=enforcing改为disabledSELINUX=disabled 校准时间 有些系统如Ubuntu安装时校准过时区，则此步骤可跳过 建议先用date命令查看时间是否有问题 大数据产生与处理系统是各种计算设备集群的，计算设备将统一、同步的标准时间用于记录各种事件发生时序，若计算机时间不同步，这些应用或操作或将无法正常进行。网络时间同步协议(NTP)是时间同步的技术基础。 linux-ntp时间同步 On the Linux system, we have the hardware and the system clocks. The hardware clock also is known as the real-time clock (RTC). It runs independently from the kernel even while the server is shut down. Whereas the system clock is maintained by the OS. During Linux boot time, the hardware clock updates the system clock. But the hardware clock is not reliable enough. For that reason, we use NTP to synchronize our system time from an external and more accurate time source. Another key point to mention is that NTP is based on the UTC time zone. Hence we need to configure our system time zone appropriately. syncing time with network on Linux 123456789# 安装ntp：yum -y install ntp# 启动并设置开机自启：systemctl start ntpdsystemctl enable ntpd# 当ntpd启动后会定期的帮助我们联网校准系统的时间# 也可以手动校准ntpdate -u ntp.aliyun.com 配置JDK环境 jdk下载地址 下载JDK安装文件，解压到&#x2F;export&#x2F;server文件夹下（自己创建一个文件夹，方便使用即可，也可通过yum安装，如通过yum安装，则目录一般在&#x2F;usr&#x2F;local下） 设置软连接，方便访问（也可直接改名，这里设置软连接方便查看版本号） (为方便整理，后续所有文件都保存在&#x2F;export&#x2F;server文件夹下) 12345export JAVA_HOME=/export/server/jdkexport PATH=$PATH:$JAVA_HOME/bin# 修改完后生效配置文件source /etc/profile 效果如图 实现ssh免密登录 how to connect without password ussing ssh 首先确保系统已安装ssh，某些系统如Ubuntu需要手动安装 123456789# 生成公钥和私钥，生成的文件在~/.ssh 下ssh-keygen -t rsa -b 4096# 复制密钥到其他服务器ssh-copy-id node1ssh-copy-id ndoe2ssh-copy-id node3# 如之前未配置hosts文件，则按照以下格式复制密钥ssh-copy-id remote_username@remote_server_ip_address 效果如下 安装Hadoophttps://hadoop.apache.org/ 基本类似jdk，下载hadoop安装包到&#x2F;export&#x2F;server下并解压，设置软连接 解压后进入hadoop文件夹 各个文件夹含义如下 • bin，存放Hadoop的各类程序（命令） • etc，存放Hadoop的配置文件（后续配置主要在这里进行） • include，C语言的一些头文件 • lib，存放Linux系统的动态链接库（.so文件） • libexec，存放配置Hadoop系统的脚本文件（.sh和.cmd） • licenses-binary，存放许可证文件 • sbin，管理员程序（super bin） • share，存放二进制源码（Java jar包) HDFS配置修改配置文件首先修改&#x2F;etc&#x2F;profile，配置HADOOP_HOME和PATH 123456# 在/etc/profile文件底部追加如下内容export HADOOP_HOME=/export/server/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin# 重新读取profile文件，使配置生效source /etc/profile 配置HDFS集群，主要涉及到如下文件的修改： workers：配置从节点（DataNode）有哪些 hadoop-env.sh：配置Hadoop的相关环境变量 core-site.xml：Hadoop核心配置文件 hdfs-site.xml：HDFS核心配置文件 注意：因为涉及到三台虚拟机，每台4个配置文件，共12个配置文件的修改，一一修改非常繁琐，建议只修改一台虚拟机的配置文件，其他两台通过scp命令分发即可 以上文件均存在于$HADOOP_HOME&#x2F;etc&#x2F;hadoop文件夹中 12345678910111213141516# workers表明DataNode所在位置#填入以下内容(如未修改hosts文件，则填入对应ip地址)node1node2node3# 填入如下内容#指明JDK环境位置export JAVA_HOME=/export/server/jdk#指明Hadoop安装位置export HADOOP_HOME=/export/server/hadoop# 指明Hadoop配置文件目录位置export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop# 知名Hadoop运行日志目录位置export HADOOP_LOG_DIR=$HADOOP_HOME/logs 12345678910111213141516171819&lt;!--在configuration内部填入如下内容--&gt;&lt;configuration&gt; &lt;!-- hdfs文件系统的网络通讯路径 协议为hdfs:// namenode为node1（未配置hosts文件则这里改为ip地址即可） 通讯端口为8020 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node1:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs:&#x2F;&#x2F;node1:8020为整个HDFS内部的通讯地址，应用协议为hdfs:&#x2F;&#x2F;（Hadoop内置协议） 表明DataNode将和node1的8020端口通讯，node1是NameNode所在机器 此配置决定node1必须启动NameNode进程、 12345678910111213141516171819202122232425262728293031323334353637383940&lt;configuration&gt; &lt;!--默认创建的文件权限设置为700--&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt; &lt;value&gt;700&lt;/value&gt; &lt;/property&gt; &lt;!--namenode元数据存储位置，文件夹需手动创建--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/data/nn&lt;/value&gt; &lt;/property&gt; &lt;!--datanode数据存储目录，文件夹需手动创建--&gt; &lt;!--namenode文件夹只需要在node1创建，而datanode三台机器上都需要创建--&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/data/dn&lt;/value&gt; &lt;/property&gt; &lt;!--namenode允许那几个节点的datanode连接，授权node1，node2，node3--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.hosts&lt;/name&gt; &lt;value&gt;node1,node2,node3&lt;/value&gt; &lt;/property&gt; &lt;!--hdfs默认块大小，256MB--&gt;\t&lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;268435456&lt;/value&gt; &lt;/property&gt; &lt;!-- namenode处理的并发线程数--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 12345678# 注意：server文件夹里是有名为hadoop的软连接和带版本号的源文件，不要传错了。如果最开始直接重命名而不是设置软连接则无需在意# 注意：是反引号`pwd`不是单引号&#x27;pwd&#x27;，反引号表示命令替换，单引号只是单纯的字符串cd /export/serverscp -r hadoop-3.3.4 node2:`pwd`/scp -r hadoop-3.3.4 node3:`pwd`/# scp简介scp -r sourceFile username@host:destpath 创建并授权hadoop用户，后续操作都在hadoop用户下执行，root用户功成身退 12345678# 创建hadoop用户adduser hadoop# 以root身份，在三台服务器上均执行，授权hadoop用户# namenode和datanode存储目录chown -R hadoop:hadoop /data# hadoop安装目录chown -R hadoop:hadoop /export 在node1中初始化文件系统，随后即可启动hdfs集群 1234# 确保以hadoop用户执行su - hadoop# 格式化namenodehadoop namenode -format 1234567# 在node1虚拟机中一键启动hdfs集群start-dfs.sh# 一键关闭hdfs集群stop-dfs.sh# 如果遇到命令未找到的错误，表明环境变量未配置好，可以以绝对路径执行/export/server/hadoop/sbin/start-dfs.sh/export/server/hadoop/sbin/stop-dfs.sh 效果如图 HDFS常用命令和linux操作命令基本相同，加上hdfs dfs 的前缀即可（或hadoop fs），以mkdir和ls为例，其他不再赘述 12345# 在根目录下递归创建/25/12文件夹hdfs dfs -mkdir -p /25/12#列出根目录下所有文件hdfs dfs -ls / YARN&amp;MapReduce配置有了hdfs配置的基础，yarn和mapreduce的配置就轻车熟路了 所有的配置文件都在$HADOOP_HOME&#x2F;etc&#x2F;hadoop 目录下 MapReduce配置文件123456# 设置JDK路径export JAVA_HOME=/export/server/jdk# 设置JobHistoryServer进程内存export HADOOP_JOB_HISTORYSERVER_HEAPSIZE=1000# 设置日志级别export HADOOP_MAPRED_ROOT_LOGGER=INFO,RFA 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;description&gt;设置mapreduce运行框架为yarn&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node1:10020&lt;/value&gt; &lt;description&gt;设置历史服务器通讯端口&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node1:19888&lt;/value&gt; &lt;description&gt;设置历史服务器web端口&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt; &lt;value&gt;/data/mr-history/tmp&lt;/value&gt; &lt;description&gt;历史信息在hdfs的记录临时路径&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt; &lt;value&gt;/data/mr-history/done&lt;/value&gt; &lt;description&gt;历史信息在hdfs的记录路径&lt;/description&gt; &lt;/property&gt;\t&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt; &lt;description&gt;mapreduce_home设置为HADOOP_HOME&lt;/description&gt;\t&lt;/property&gt;\t&lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;\t&lt;/property&gt;\t&lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;\t&lt;/property&gt;&lt;/configuration&gt; Yarn配置文件1234export JAVA_HOME=/export/server/jdkexport HADOOP_HOME=/export/server/hadoopexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport HADOOP_LOG_DIR=$HADOOP_HOME/logs 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;property&gt;\t&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\t&lt;value&gt;node1&lt;/value&gt;\t&lt;discription&gt;ResourceManager设置在node1节点&lt;/discription&gt;&lt;/property&gt;&lt;property&gt;\t&lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;\t&lt;value&gt;/data/nm-local&lt;/value&gt;\t&lt;discription&gt;NodeManager中间数据本地存储路径&lt;/discription&gt;&lt;/property&gt;&lt;property&gt;\t&lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;\t&lt;value&gt;/data/nm-log&lt;/value&gt;\t&lt;discription&gt;NodeManager数据日志本地存储路径&lt;/discription&gt;&lt;/property&gt;&lt;property&gt;\t&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\t&lt;value&gt;mapreduce_shuffle&lt;/value&gt;\t&lt;discription&gt;为MapReduce程序开启Shuffle服务&lt;/discription&gt;&lt;/property&gt;&lt;property&gt;\t&lt;name&gt;yarn.log.server.url&lt;/name&gt;\t&lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;\t&lt;discription&gt;历史服务器URL&lt;/discription&gt;&lt;/property&gt;&lt;property&gt;\t&lt;name&gt;yarn.web-proxy.address&lt;/name&gt;\t&lt;value&gt;node1:8089&lt;/value&gt;\t&lt;discription&gt;代理服务器主机和端口&lt;/discription&gt;&lt;/property&gt;&lt;property&gt;\t&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;\t&lt;value&gt;true&lt;/value&gt;\t&lt;discription&gt;开启日志聚合&lt;/discription&gt;&lt;/property&gt;&lt;property&gt;\t&lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;\t&lt;value&gt;/tmp/logs&lt;/value&gt;\t&lt;discription&gt;程序日志HDFS的存储路径&lt;/discription&gt;&lt;/property&gt;&lt;property&gt;\t&lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;\t&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;\t&lt;discription&gt;选择公平调度器&lt;/discription&gt;&lt;/property&gt; 在node1中修改完成后用scp命令分发到其他服务器中即可 12scp mapred-env.sh mapred-site.xml yarn-env.sh yarn-site.xml node2:`pwd`/scp mapred-env.sh mapred-site.xml yarn-env.sh yarn-site.xml node3:`pwd`/ 启动并提交任务启动HDFS 一键启停脚本可用 $HADOOP_HOME&#x2F;sbin&#x2F;start-dfs.sh $HADOOP_HOME&#x2F;sbin&#x2F;stop-dfs.sh 独立进程启停可用 $HADOOP_HOME&#x2F;sbin&#x2F;hadoop-daemon.sh $HADOOP_HOME&#x2F;bin&#x2F;hdfs –daemon YARN 一键启动YARN集群： $HADOOP_HOME&#x2F;sbin&#x2F;start-yarn.sh 会基于yarn-site.xml中配置的yarn.resourcemanager.hostname来决定在哪台机器上启动resourcemanager 会基于workers文件配置的主机启动NodeManager 一键停止YARN集群： $HADOOP_HOME&#x2F;sbin&#x2F;stop-yarn.sh 在当前机器，单独启动或停止进程：$HADOOP_HOME&#x2F;bin&#x2F;yarn –daemon start|stop resourcemanager|nodemanager|proxyserver start和stop决定启动和停止 可控制resourcemanager、nodemanager、proxyserver三种进程 历史服务器启动和停止：$HADOOP_HOME&#x2F;bin&#x2F;mapred –daemon start|stop historyserver 在node1中，启动hdfs，yarn 提交MadReduce任务Hadoop官方提供了一些预置的MapReduce程序，可以直接使用 可以通过 hadoop jar 命令来运行程序，提交MapReduce程序到YARN中 12# 语法hadoop jar 程序文件 java类名 [程序参数] … [程序参数] 这里以examples jar包中的wordcount为例，源代码可在如下地址查看 https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html wordcountwordcount功能很简单： 给定数据输入的路径（HDFS）、给定结果输出的路径（HDFS） 将输入路径内的数据中的单词进行计数，将结果写到输出路径 123hadoop jar 程序文件 java类名 [程序参数] ... [程序参数]hadoop jar hadoop-mapreduce-examples-3.3.4.jar wordcount hdfs://node1:8020/input/wordcount hdfs://node1:8020/output/wd 输入为&#x2F;input&#x2F;wordcount文件夹，该文件夹下只有一个文件如下（mapreduce以目录为单位进行统计，并非以文件为单位） 输出在&#x2F;output&#x2F;wd文件夹下，一个标识是否成功的标识文件，一个结果文件，如下 也可在yarn WEB UI查看任务调度","categories":["大数据","hadoop"]},{"title":"简易博客搭建：hexo+gitpages+picgo+typora","path":"/2024/02/15/简易博客搭建：hexo+gitpages+picgo+typora/","content":"博客在2024年可能已经是一个古老的名词了，大家都早已习惯从抖音、小红书、B站、知乎等各类APP上获取信息了，但是所有的社区用户都受困于审核、信息茧房以及越来越极端的网络环境，一个独属于自己的、自由的小花园愈发可贵。 更何况作为程序员，一个技术博客也是很好的整理、反思和交流的方式 本文会详细介绍如何用hexo+github pages+picgo+typora实现0成本、易上手、无写作门槛的个人博客。 hexo快速入门hexo官方中文文档：Hexo 安装安装前提 Node.js Git 安装hexo1234$ npm install -g hexo-cli//安装成功后查看versionhexo version 建站hexo的使用非常简单 123$ hexo init &lt;folder&gt;//所有文件会在&lt;folder&gt;下自动创建$ cd &lt;folder&gt; 建成后目录结构如下 12345678.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes _source&#x2F;_posts里就是博客内容，一篇博客就是一个markdown文件 最初里面只有一个hello-world.md文件 hexo提供了简单的命令将md文件转成静态页面 12345//生成静态页面hexo g//本地查看静态页面hexo s 浏览器访问http://localhost:4000 就可以打开博客了，现在里面只有一篇文章，_source&#x2F;_posts下的hello-world.md 部署gitpages现在我们还只能在本地访问自己的博客，但个人博客最终一定是要部署到互联网上让所有人都能看到的，就像所有人都能访问www.baidu.com 一样 通过GitHub Pages可以很轻易的实现 在github上新建一个仓库，名为username.github.io !注意:这里的仓库名不要轻易改变，会影响到最终的博客网址，标准设置的仓库名最终可以简单的通过https://username.github.io/ 访问，否则访问地址会是全路径还可能导致css样式失效等等问题 然后修改 _config.yml 文件，找到 Deployment 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: &#x27;替换为github仓库的ssh地址&#x27; branch: master 更复杂的需求见官方文档：配置 | Hexo 最后安装hexo-deployer-git插件，用于将blog部署到 GitHub 上面 1npm install hexo-deployer-git --save 部署在 blog 目录下，分别输入如下三条命令： 1234567hexo clean //清除缓存//生成的静态文件在public文件夹下hexo g //生成静态文件//部署就是把生成的静态文件上传到上面的github仓库里hexo d //部署 最后进入settings界面，开启github pages 访问https://username.github.io/即可 总结写作流程非常简单 在_source&#x2F;_posts文件夹下创建markdown文件 hexo g生成静态页面 hexo s本地预览页面是否正常（GitHub pages会自动更新，但是更新有延迟，所以日常建议先本地预览，再deploy到GitHub） hexo d将静态页面上传到github 仓库 访问github pages 个性化hexo提供了丰富的主题插件：Themes | Hexo 主题作者都提供了详细的安装说明和演示页面，这里不再赘述 我用的是stellar：Stellar：开始您全新的博客之旅 - XAOXUU 图床：PicGo+GitHubmarkdown不是MS word那样的富文本编辑器，markdown文件是不能内嵌图片的，只能通过链接的方式引用 一般本地写作markdown都需要用一个额外的文件夹保存png&#x2F;jpg文件，然后通过相对路径或者绝对路径引用 但是部署在网上的博客是不存在本地文件夹的，所以需要一个互联网上的文件夹，也就是图床 这里同样用github pages搭建一个免费0成本的图床 配置github 创建github 仓库 仓库里添加一个简单的index.html文件，否则不会提供GitHub pages的功能 开启GitHub pages 配置PicGo 下载PicGo PicGo is Here | PicGo 配置github 图床 需要访问 https://github.com/settings/tokens 获取token 上传一张图片测试 图片上传完成之后会返回一个markdown的可用链接，直接粘贴到markdown编辑器中就可以预览图片了 配置typora但是这样一个一个手动上传图片然后粘贴到markdown编辑器里太麻烦了 个人推荐使用typora，提供了集成PicGo的功能，无需任何额外操作，直接粘贴图片，typora会自动上传图片、转换图片路径为图库路径、所见即所得，非常好用 写作Front-matter | Hexo 文章标签、分类在新建的文章开头编辑文章的题目，作者，日期，标签等。 12345678910---title: testdate: 2020-10-17 17:24:36author: hiyocopyright: truetags: - 标签1 - 标签2categories: 分类--- typora中建议在源码模式下进行编辑这部分内容 子分类将该文章放到 Sports&#x2F;Baseball 这个分类下。 123categories: - Sports\t- Baseball 或者： 12categories:\t- [Sports,Baseball] 多个分类将文章同时分到两个或者多个不同的类目下 123456categories: - [Sports] - [Play]categories: - [Sports,Baseball] - [Play] 总结hexo+gitpages+picgo+typora基本可以实现0部署成本+低写作门槛，不过个人博客的意义还是在于内容本身，我今后也会把学习工作中的心得和文档整理到博客上分享交流，立个小flag，三天一更新。","categories":["随笔"]}]